{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data in Your Dataset\n",
    "\n",
    "Consider the following situation -\n",
    "\n",
    "You are working on your dataset. You create a classification model and get 90% accuracy immediately. The results seem fantastic to you. You dive a little deeper and discover that almost entirety of the data belongs to one class. Damn! Imbalanced data can cause you a lot of frustration.\n",
    "\n",
    "You feel very frustrated when you discovered that your data has imbalanced classes and that all of the great results you thought you were getting turn out to be a lie. What is even more frustrating is the good books don't even holistically cover this topic.\n",
    "\n",
    "This is an example of a situation cases by an imbalanced dataset and the frustrating results it can cause.\n",
    "\n",
    "In this tutorial, you will discover the techniques that you can use to deliver excellent results on datasets with imbalanced data. Specifically, you will cover:\n",
    "\n",
    "- What is meant by imbalanced data?\n",
    "- Why are imbalanced datasets a serious problem to tackle?\n",
    "- Accuracy Paradox\n",
    "- Different metrics for classifier evaluation\n",
    "- Various approaches to handling imbalanced data\n",
    "- Further reading on the topic\n",
    "\n",
    "Let's first see what imbalanced data is.\n",
    "\n",
    "<img src = 'https://www.kdnuggets.com/wp-content/uploads/imbalanced-data-1.png' />\n",
    "Image Source: KDNuggets\n",
    "\n",
    "__NOTE__: Most datasets in the real world are somewhat imbalanced; it is always a good practice to include the investigation of the target distribution in your EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is imbalanced data?\n",
    "Imbalanced data typically refers to classification tasks where the classes are not represented equally.\n",
    "\n",
    "For example, you may have a binary classification problem with `100` instances out of which `80` instances are labeled with `Class-1`, and the remaining `20` instances are marked with `Class-2`.\n",
    "\n",
    "This is essentially an example of an imbalanced dataset, and the ratio of `Class-1` to `Class-2` instances is `4:1`.\n",
    "\n",
    "Be it a Kaggle competition or real test dataset, the class imbalance problem is one of the most common ones.\n",
    "\n",
    "Most of the real-world classification problems display some level of class imbalance, which happens when there are not sufficient instances of the data that correspond to either of the class labels. Therefore, it is imperative to choose the evaluation metric of your model correctly. If it is not done, then you might end up adjusting/optimizing a useless parameter. In a real business-first scenario, this may lead to a complete waste.\n",
    "\n",
    "There are problems where a class imbalance is not just common; it is bound to happen. For example, the datasets that deal with fraudulent and non-fraudulent transactions, it is very likely that the number of fraudulent transactions as compares to the number of non-fraudulent transactions will be very much less. And this is where the problem arises. You will study why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are imbalanced datasets a serious problem to tackle?\n",
    "Although all many machine learning algorithms (both deep and statistical) have shown great success in many real-world applications, the problem of learning from imbalanced data is still yet to be state-of-the-art. And often, this learning from imbalanced data is referred to as **Imbalanced learning**.\n",
    "\n",
    "Following are the significant problems of imbalanced learning:\n",
    "\n",
    "- When the dataset has underrepresented data, the class distribution starts skew.\n",
    "- Due to the inherent complex characteristics of the dataset, learning from such data requires new understandings, new approaches, new principles, and new tools to transform data. And moreover, this cannot anyway guarantee an efficient solution to your business problem. In worst cases, it might turn to complete wastes with zero residues to reuse.\n",
    "\n",
    "At this point of time one obvious question that must have come to your mind is - why in an age of GPUs, TPUs machine learning algorithms are failing to tackle imbalanced data efficiently? Quite an obvious question and you will find its answer now.\n",
    "\n",
    "Evaluation of machine learning algorithms has a lot to do as to the reason why a particular machine learning algorithm does not perform when supplied with imbalanced data.\n",
    "\n",
    "\"It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution.\" - [Machine Learning Mastery](https://www.datacamp.com/community/tutorials/%22http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/%22)\n",
    "\n",
    "Suppose, you have a dataset (associated with a classification task) with two classes with a distribution ratio of `9:1`. The total number of instances present in the dataset is `1000`, and the class labels are `Class-1` and `Class-2`. Therefore, w.r.t the distribution ratio, the number of instances that correspond to `Class-1` is `900` while `Class-2` instances are `100`. Now, you applied a standard classifier (say `Logistic Regression`) and measured its performance concerning classification accuracy which gives the number of instances correctly classified by the classifier. Now, take a closer look and think very deeply.\n",
    "\n",
    "You Logistic Regression model does not have to be very complicated to classify all the 1000 instances as `Class-1`. In that case, you would get a classification accuracy of `90%` which is really not enough to test the actual quality of the classifier. Clearly, you need some other metric to evaluate the performance of the system. You will see that in a minute. The phenomenon you just studied is called **Accuracy Paradox**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches for handling imbalanced data:\n",
    "You will start this section by studying some metrics other than classification accuracy in order to truly judge a classifier when it is dealing with imbalanced data.\n",
    "\n",
    "First, let's define four fundamental terms here:\n",
    "\n",
    "- **True Positive (TP)** – An instance that is positive and is classified correctly as positive\n",
    "- **True Negative (TN)** – An instance that is negative and is classified correctly as negative\n",
    "- **False Positive (FP)** – An instance that is negative but is classified wrongly as positive\n",
    "- **False Negative (FN)** – An instance that is positive but is classified incorrectly as negative\n",
    "\n",
    "The following image will justify the above terms for themselves:\n",
    "<img src = 'http://www.chioka.in/wp-content/uploads/2013/08/Metrics-Table-580x280.png' />\n",
    "[Image Source](http://www.chioka.in/class-imbalance-problem/)\n",
    "\n",
    "Now, assume that you trained another classifier on the toy dataset you just saw and this time you applied a Random Forest. And you got a classification accuracy of `70%`. Now that, you know about True Positive and Negative Rates and False Positive and Negative Rates, you will investigate the performance of the earlier `Logistic Regression` and `Random Forest` in a bit more detailed manner.\n",
    "\n",
    "Suppose you got the following True Positive and Negative Rates and False Positive and Negative Rates for the `Logistic Regression` model:\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1539113868/Capture1_l9fvnu.png />\n",
    "\n",
    "Now, assume that the True Positive and Negative Rates and False Positive and Negative Rates for the `Random Forest` model are following:\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1539113868/Capture2_ol19nx.png />\n",
    "\n",
    "Just look at the number of **negative classes** correctly predicted (**True Negatives**) by both of the classifiers. As you are dealing with an imbalanced dataset, you need to give this number the most priority (because `Class-1` dominant in the dataset). So, considering that, `Random Forest` trades away `Logistic Regression` easily.\n",
    "\n",
    "Now, you are in an excellent place to study the approaches for combating imbalanced dataset problem.\n",
    "\n",
    "(Remember, the above representation is popularly known as the **Confusion Matrix**.)\n",
    "\n",
    "Following are the two terms that are derived from the confusion matrix and very much used when you are evaluating a classifier.\n",
    "\n",
    "**Precision**: Precision is the number of **True Positives** divided by the number of True Positives and False Positives. Put another way; it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV).\n",
    "\n",
    "Precision can be thought of as a measure of a classifier's *exactness*. A low precision can also indicate **a large number of False Positives**.\n",
    "\n",
    "**Recall**: Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate.\n",
    "\n",
    "Recall can be thought of as a measure of a classifier's *completeness*. A low recall indicates many **False Negatives**.\n",
    "\n",
    "Some other metrics that can be useful in this context:\n",
    "\n",
    "- [AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "- [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "- [f1-Score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- [Matthews correlation coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
    "\n",
    "Before you begin studying the approaches to tackle class-imbalance problem let's take a very real-world example where just choosing classification accuracy as the evaluation metric can produce disastrous results. (You are doing this to ensure you do not only consider classification accuracy when you train your next classifier.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Case Study\n",
    "\n",
    "The [breast cancer dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer) is a standard machine learning dataset. It contains `9` attributes describing 699 women that have suffered and survived breast cancer and whether or not breast cancer recurred within 5 years. Let's investigate this dataset to get you a real feel of the problem.\n",
    "\n",
    "The dataset concerns a binary classification problem. Of the `699` women, `458` did not suffer a recurrence of breast cancer, leaving the remaining `241` that did.\n",
    "\n",
    "Let's explore about the dataset more visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Specify Column Names\n",
    "cols = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "       'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli',\n",
    "       'Mitoses', 'Class']\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "data = pd.read_csv('./data/cancer.csv', names=cols)\n",
    "#data.head()\n",
    "\n",
    "# See the data\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[data.Class == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a bar graph of the class distributions. You can always use Pie Chart or other visualization tools for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "classes = data['Class'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "\n",
    "plt.bar(unique,counts)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the class imbalance here. `4` denotes the instances which have cancer and is obvious, the number of these instances is much lower as compared to the instances corresponding to the other class.\n",
    "\n",
    "Let's define \"No Recurrences\" and \"Recurrences\" event that is there in the dataset which will make things even more evident.\n",
    "\n",
    "- *All No Recurrence*: A model that only predicted no recurrence of breast cancer would achieve an accuracy of $(458/699) * 100%$ or 65.52%. This is called All No Recurrence. This is a high accuracy, but a terrible model. If this model was misinterpreted, it would send home 241 women incorrectly thinking their breast cancer was not going to reoccur (High False Negatives).\n",
    "\n",
    "- All recurrence: A model that only predicted the recurrence of breast cancer would achieve an accuracy of $(241/699) * 100%$ or 34.48%. This is known All Recurrence. This model fails at producing good accuracy and would send home 458 women thinking that had a recurrence of breast cancer, but this is not the case (High False Positives).\n",
    "\n",
    "This concept should have sparked an ignition inside you by now. Let's move forward with it.\n",
    "\n",
    "Well! Now, you have now enough reasons to wonder why considering only classification accuracy to evaluate your classification model is not a good choice.\n",
    "\n",
    "Let's study some approaches now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-sampling the dataset:\n",
    "Dealing with imbalanced datasets includes various strategies such as **improving classification algorithms** or **balancing classes in the training data** (essentially a data preprocessing step) before providing the data as input to the machine learning algorithm. The **latter** technique is preferred as it has broader application and adaptation. Moreover, the time taken to enhance an algorithm is often higher than to generate the required samples. But for research purposes, both are preferred.\n",
    "\n",
    "__NOTE__: We ONLY balance the __training__ dataset.\n",
    "\n",
    "The main idea of sampling classes is to either increasing the samples of the minority class or decreasing the samples of the majority class. This is done in order to obtain a fair balance in the number of instances for both the classes.\n",
    "\n",
    "There can be two main types of sampling:\n",
    "\n",
    "- You can add copies of instances from the minority class which is called over-sampling (or more formally sampling with replacement), or\n",
    "- You can delete instances from the majority class, which is called under-sampling.\n",
    "\n",
    "This sounds even easier from the implementation perspective as well. Isn't it? Later in this post, you will get to know about a library dedicated to performing sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random under-sampling:\n",
    "When you randomly eliminate instances from the majority class of a dataset and assign it to the minority class (without filling out the void created in majority class), it is known as random under-sampling. The void that gets created in the majority dataset for this makes the process random.\n",
    "\n",
    "#### Advantages of this approach:\n",
    "\n",
    "- It can help improve the runtime of the model and solve the memory problems by reducing the number of training data samples when the training data set is enormous.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "- It can discard useful information about the data itself which could be necessary for building rule-based classifiers such as Random Forests.\n",
    "- The sample chosen by random undersampling may be a biased sample. And it will not be an accurate representation of the population in that case. Therefore, it can cause the classifier to perform poorly on real unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random over-sampling:\n",
    "Just like random under-sampling, you can perform random oversampling as well. But in this case, taking any help from the majority class, you increase the instances corresponding to the minority class by replicating them up to a constant degree. In this case, you do not decrease the number of instances assigned to the majority class. Say, you have a dataset with `1000` instances where `980` instances correspond to the majority class, and the reaming `20` instances correspond to the minority class. Now you over-sample the dataset by replicating the `20` instances up to `20` times. As a result, after performing over-sampling the total number of instances in the minority class will be `400`.\n",
    "\n",
    "#### Advantages of random over-sampling:\n",
    "\n",
    "- Unlike undersampling, this method leads to no information loss.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "- It increases the likelihood of overfitting since it replicates the minority class events.\n",
    "\n",
    "You can consider the following factors while thinking of applying under-sampling and over-sampling:\n",
    "\n",
    "- Consider applying under-sampling when you have a lot of data\n",
    "- Consider applying over-sampling when you don’t have a lot of data\n",
    "- Consider applying random and non-random (e.g., stratified) sampling schemes.\n",
    "- Consider applying different ratios of the class-labels (e.g., you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)\n",
    "\n",
    "If you want to implement under-sampling and over-sampling in Python, you should check out [scikit-learn-contrib](https://github.com/scikit-learn-contrib/imbalanced-learn).\n",
    "\n",
    "__NOTE__: if you are into over-sampling, currently I am working on a project using __Deep Learning__ techniques to create an over-sampling technique (namely SMOTE, discussed below). Please contact me if you are interested.\n",
    "\n",
    "Now you will study the next approach for handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try generating synthetic samples:\n",
    "\n",
    "A simple way to create synthetic samples is to sample the attributes from instances in the minority class randomly.\n",
    "\n",
    "There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique. It was proposed in 2002, and you can take a look at the [original SMOTE paper](https://www.datacamp.com/community/tutorials/%22http://www.jair.org/papers/paper953.html%22). Following info-graphic will give you a fair idea about the synthetic samples:\n",
    "\n",
    "<img src = 'https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png' />\n",
    "\n",
    "[Image Source](https://cdn-images-1.medium.com/max/1600/1*uAiwqUNhqaSZmsXCrl9kVQ.png%22)\n",
    "\n",
    "SMOTE is an oversampling method which creates “synthetic” example rather than oversampling by replacements. The minority class is **over-sampled** by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen.\n",
    "\n",
    "<img src = 'https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/03/16145123/ICP9.png' />\n",
    "[SMOTE Workflow](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)\n",
    "\n",
    "**NOTE**: In other words, instead of **copying** instances in the minority class, new instances **similar to** the original instances are generated.\n",
    "\n",
    "The heart of SMOTE is the construction of the minority classes. The intuition behind the construction algorithm is simple. You have already studied that oversampling causes overfitting, and because of repeated instances, the decision boundary gets tightened. What if you could generate similar samples instead of repeating them? In the original SMOTE paper (linked above) it has been shown that to a machine learning algorithm, these newly constructed instances are not exact copies, and thus it softens the decision boundary and thereby helping the algorithm to approximate the hypothesis more accurately.\n",
    "\n",
    "However, there are some advantages and disadvantages of SMOTE -\n",
    "\n",
    "#### Advantages -\n",
    "\n",
    "- Alleviates overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances.\n",
    "- No loss of information.\n",
    "- It's simple to implement and interpret.\n",
    "\n",
    "#### Disadvantages -\n",
    "\n",
    "- While generating synthetic examples, SMOTE does not take into consideration neighboring examples can be from other classes. This can increase the overlapping of classes and can introduce additional noise.\n",
    "- SMOTE is not very practical for high dimensional data.\n",
    "\n",
    "There are some variants of SMOTE such as safe-level SMOTE, border-line SMOTE, OSSLDDD-SMOTE, etc. If you want to use SMOTE and its other variants you can check the scikit-learn-contrib module as mentioned before. If you want to learn more about SMOTE, you should check [this](https://www.datacamp.com/community/tutorials/sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf) and [this](https://rd.springer.com/chapter/10.1007/978-3-642-01307-2_43) papers.\n",
    "\n",
    "Let's proceed to the final approach for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out different perspectives:\n",
    "There are fields of study dedicated to handling imbalanced datasets. They have their own algorithms, measures, and terminology.\n",
    "\n",
    "Often creativity and an innovative bent of mind can give you new perspectives to consider when you are dealing with imbalanced data. The following approach can give you a head-start:\n",
    "\n",
    "Cost-Sensitive Learning: Generally, you use regularization (if you want to learn more about regularization you can check this DataCamp article) for penalizing large coefficients which often appear in Generalized Linear Models (GLM). Although this application varies from model to model. You are considering GLM only for the time being. If you can devise a mechanism which penalizes the classifier each time it does a misclassification. This can help the classifier to learn the hypothesis in a much more detailed manner.\n",
    "\n",
    "Following are some approaches that you should consider:\n",
    "\n",
    "- Use K-fold Cross-Validation in the right way\n",
    "- Ensemble different re-sampled datasets\n",
    "- Resample with different ratios\n",
    "- Cluster the abundant class\n",
    "\n",
    "\n",
    "### Wrap up!\n",
    "So far, you got yourself introduced to the concept of imbalanced data and the kind of problem it creates while designing and developing machine learning models. You also saw several reasons as to why it is crucial to tackle imbalanced data. After that, you studied different approaches that can help you to handle imbalanced datasets effectively. Processing imbalanced data is an active area of research, and it can open new horizons for you to consider new research problems.\n",
    "\n",
    "A lot of essential concepts in one go! Absolutely amazing!\n",
    "\n",
    "That is all for this tutorial.\n",
    "\n",
    "Below are some paper links if you are very keen to study even more about the topic of imbalanced data:\n",
    "\n",
    "- [Learning from Imbalanced Data](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907)\n",
    "- [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection](http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf)\n",
    "- [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](http://dl.acm.org/citation.cfm?id=1007735)\n",
    "\n",
    "### References:\n",
    "\n",
    "- [Analytics Vidhya article on imbalanced data](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)\n",
    "- [Towards Data Science article on Imbalanced data](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
